{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[19-1] CNN을 이용한 리뷰 감성 분석.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOr9qnzlyZ0p6QCSvFzAiQf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WKGN5psV0lkc","colab_type":"text"},"source":["**1. 데이터 다운**  \n","https://www.kaggle.com/bittlingmayer/amazonreviews"]},{"cell_type":"markdown","metadata":{"id":"YwBIxeVU01Sw","colab_type":"text"},"source":["**2. 패키지 선언**"]},{"cell_type":"code","metadata":{"id":"6JU8LL3_zVuB","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import nltk\n","import random\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pp7UFXs504nP","colab_type":"text"},"source":["**3. 학습 데이터와 평가 데이터**\n","\n"]},{"cell_type":"code","metadata":{"id":"ZD7A9WfU09Hp","colab_type":"code","colab":{}},"source":["def read_dataset(dataset_type):\n","  max_seq_len = 0\n","  with open(\"/content/%s.txt\" % dataset_type, \"r\", encoding=\"utf-8\") as fr_handle:\n","    labels, sentences, tokenized_sentences = [], [], []\n","    for line in fr_handle:\n","      if line.strip() == 0:\n","        continue\n","      label = line.split(' ')[0]\n","      label = 0 if label == \"__label__1\" else 1 # 부정이면 0, 긍정이면 1\n","\n","      sentence = ' '.join(line.split(' ')[1:])\n","      tokenized_sentence = nltk.word_tokenize(sentence)\n","      max_seq_len = max(max_seq_len, len(tokenized_sentence))\n","\n","      labels.append(label)\n","      sentences.append(sentence)\n","    \n","    return labels, sentences, max_seq_len\n","\n","TRAIN_LABELS, TRAIN_SENTENCES, TRAIN_MAX_SEQ_LEN = read_dataset(\"train\")\n","TEST_LABELS, TEST_SENTENCES, TEST_MAX_SEQ_LEN = read_dataset(\"test\")\n","MAX_SEQUENCE_LEN = max(TRAIN_MAX_SEQ_LEN, TEST_MAX_SEQ_LEN)\n","\n","print(\"Train : \", len(TRAIN_SENTENCES))\n","for train_label, train_sent in zip(TRAIN_LABELS, TRAIN_SENTENCES[0:10]):\n","  print(train_label, ':' ,train_sent)\n","\n","print()\n","print(\"Test : \", len(TEST_SENTENCES))\n","for test_label, test_sent in zip(TEST_LABELS, TEST_SENTENCES[0:10]):\n","  print(test_label, ':' ,test_sent)\n","\n","print(\"MAX_SEQUENCE_LEN\", MAX_SEQUENCE_LEN)\n","with open(\"/content/vocab.txt\", \"r\", encoding=\"utf-8\") as vocab_handle:\n","  VOCAB = [line.strip() for line in vocab_handle if len(line.strip()) > 0]\n","  \n","  print(\"Total vocabulary\", VOCAB)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNZ-DfJA1R1x","colab_type":"text"},"source":["**4. Keras를 통한 전처리 과정**  \n","1) Text를 tokenize하여 id 값으로 변경해 줍니다. (tokenizer.texts_to_sequences)  \n","2) id로 변경해준 문장들을 모두 문장 최대 길이로 padding 처리해 줍니다. (pad_sequences)\n"]},{"cell_type":"code","metadata":{"id":"sbXuUUUm1Wf-","colab_type":"code","colab":{}},"source":["tokenizer = Tokenizer(num_words=len(VOCAB), lower=True, char_level=False)\n","tokenizer.fit_on_texts(TRAIN_SENTENCES)\n","TRAIN_SEQUENCES = tokenizer.texts_to_sequences(TRAIN_SENTENCES)\n","TEST_SEQUENCES = tokenizer.texts_to_sequences(TEST_SENTENCES)\n","VOCAB_SIZE = len(tokenizer.word_index) + 1\n","\n","print(TRAIN_SENTENCES[2])\n","print(TRAIN_SEQUENCES[2])\n","\n","X_train = pad_sequences(TRAIN_SEQUENCES, padding='post', maxlen=MAX_SEQUENCE_LEN)\n","X_test = pad_sequences(TEST_SEQUENCES, padding='post', maxlen=MAX_SEQUENCE_LEN)\n","print(\"PAD_SEQUENCES COMPLETES\")\n","print(X_train[0])\n","print(MAX_SEQUENCE_LEN)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QoeTSwkc1ffI","colab_type":"text"},"source":["**5. 모델 설정**  \n","1) Random으로 초기화된 임베딩이 아닌 pre-trained 된 GLoVE 임베딩으로 학습하고자 합니다.  \n","2) 따라서 학습 코퍼스에 있는 단어들 중 GLoVE 임베딩에 있는 단어들을 GLoVE 임베딩으로 초기화 해줍니다.  \n","3) 본 실험에서는 GLoVE 임베딩 크기가 50인 것과 100인 것을 통해 실험을 진행해 봅니다.  "]},{"cell_type":"code","metadata":{"id":"IVafgDKW1lL7","colab_type":"code","colab":{}},"source":["def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype=np.float32)[:embedding_dim]\n","\n","    return embedding_matrix\n","\n","EMBEDDING_DIM = 50 #100\n","embedding_matrix = create_embedding_matrix(\n","    '/content/glove.6B.50d.txt',\n","    tokenizer.word_index, EMBEDDING_DIM\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C-CL-YA-1vKa","colab_type":"text"},"source":["**6. Accuracy와 Loss 시각화**  "]},{"cell_type":"code","metadata":{"id":"TA6q5vn3145r","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","def plot_history(history):\n","    acc = history.history['acc']\n","    val_acc = history.history['val_acc']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4jiG5zL19Bq","colab_type":"text"},"source":["**7. CNN 모델 선언**  \n","1) Convolution 필터를 앞서 배운 것처럼 여러 개를 사용해 봅시다.  \n","2) 본 모델은 필터의 Window Size가 2, 3, 4, 5인 필터 각 100개씩을 사용해 모델을 학습합니다. 다양한 크기의 필터를 사용하면 성능이 더 올라갈까요?  \n","3) 정답은 \"올라갑니다.\" 입니다. Convolution 필터가 보는 단어의 갯수가 다양하게 되기 때문에 문장의 local 정보와 global 정보 모두를 학습할 수 있게 됩니다. 그럼 실험을 통해 확인해 볼까요?  "]},{"cell_type":"code","metadata":{"id":"BFATjiXK2Dqu","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras import layers\n","from keras.models import Model\n","\n","seq_input = layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n","seq_embedded = layers.Embedding(VOCAB_SIZE, \n","                           EMBEDDING_DIM, \n","                           weights=[embedding_matrix], \n","                           input_length=MAX_SEQUENCE_LEN, \n","                           trainable=True)(seq_input)\n","\n","filters = [2,3,4,5]\n","conv_models = []\n","for filter in filters:\n","  conv_feat = layers.Conv1D(filters=100, \n","                            kernel_size=filter, \n","                            activation='relu',\n","                            padding='valid')(seq_embedded)\n","  pooled_feat = layers.GlobalMaxPooling1D()(conv_feat)\n","  conv_models.append(pooled_feat)\n","\n","conv_merged = layers.concatenate(conv_models, axis=1)\n","\n","model_output = layers.Dropout(0.2)(conv_merged)\n","model_output = layers.Dense(10, activation='relu')(model_output)\n","logits = layers.Dense(1, activation='sigmoid')(model_output)\n","\n","model = Model(seq_input, logits)\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","model.summary()\n","\n","#학습 시작\n","history = model.fit(X_train, TRAIN_LABELS,\n","                    epochs=10,\n","                    verbose=True,\n","                    validation_data=(X_test, TEST_LABELS),\n","                    batch_size=128)\n","# 결과 시각화\n","plot_history(history)"],"execution_count":0,"outputs":[]}]}