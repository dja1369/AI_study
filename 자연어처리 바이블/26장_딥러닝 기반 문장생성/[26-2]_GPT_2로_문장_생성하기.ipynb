{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실습 [26-2] GPT-2로 문장 생성하기.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1F1J9gHDQx4",
        "colab_type": "text"
      },
      "source": [
        "## 실습 [26-2]  \n",
        "### 1. 실습명 : GPT-2로 문장 생성하기\n",
        "### 2. 실습 목적 및 설명\n",
        "* GPT-2-Simple 코드를 이용해 GPT-2 Medium 모델을 파인튜닝 해본다.\n",
        "* nltk에서 제공하는 영화 리뷰 데이터셋을 이용하여 문장을 생성한다.\n",
        "* 입력 데이터로는 어떠한 전처리도 거치지 않은 원본 텍스트를 사용한다.\n",
        "\n",
        "### 3. 관련 장(챕터) : 26.2.1 OpenGPT-2를 이용한 문장 생성\n",
        "### 4. 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V76SvYTXEhBJ",
        "colab_type": "code",
        "outputId": "a4b02f38-629f-4d57-8aa8-f36b943d7be7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "\"\"\"\n",
        "본 실습에서는 GPT-2 Medium 모델(355M)로 문장 생성을 진행한다. \n",
        "GPT-2 모델은 전체 파라미터 훈련에 상당한 시간이 소요되지만 Colab 환경 특성상 런타임 제약이 존재하므로 \n",
        "간단히 사전 학습된 모델을 불러와 특정 데이터셋으로 파인튜닝(fine-tuning) 후 문장을 생성하도록 한다. \n",
        "코드는 MIT의 Max Woolf(@minimaxir)가 배포한 gpt-2-simple 모델을 사용한다. \n",
        "gpt-2-simple 모델을 통해 OpenAI에서 배포한 GPT-2모델을 간편하게 사용할 수 있다. \n",
        "gpt-2-simple의 원본 소스 코드는‘https://github.com/minimaxir/gpt-2-simple'에서 확인할 수 있다.\n",
        "\"\"\"\n",
        "# Colab 환경에서 실행할 경우 빠른 모델 훈련을 위해 런타임 유형을 GPU로 설정하는 것이 좋다.\n",
        "# gpt-2-simple의 경우 현재 텐서플로우2.0을 지원하지 않으므로 tensorflow_version 1.x로 설정한다. \n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "import gpt_2_simple as gpt2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwG75pItF_uU",
        "colab_type": "code",
        "outputId": "6b14f65c-9338-4e81-bc43-e320a5345563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# 데이터셋을 가져와 내용을 확인한다.\n",
        "raw_data = movie_reviews.raw()\n",
        "print(raw_data[:150])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "plot : two teen couples go to a church party , drink and then drive . \n",
            "they get into an accident . \n",
            "one of the guys dies , but his girlfriend continue\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFQ6RUtRT5aE",
        "colab_type": "code",
        "outputId": "eea7a716-c038-4c4f-8632-c5fc0741cbbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "ls /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoint\u001b[0m/  \u001b[01;34mmodels\u001b[0m/  move_review_file.txt  movie_review_file.txt  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9vLVqYfGONE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋을 txt 파일로 만들어 현재 위치(/content/)에 저장한다.\n",
        "data_file = \"movie_review_file.txt\"\n",
        "with open(data_file, 'w') as f:\n",
        "  f.write(raw_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvNOCZJHSd4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5w1Ka0sOO8C",
        "colab_type": "code",
        "outputId": "4eddeaa1-061d-4656-d219-8e00e4f3dd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# GPT-2 모델을 다운로드 받은 후 데이터셋 파일을 이용해 파인 튜닝을 진행한다.\n",
        "model_name = \"355M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "  print(f\"Downloading {model_name} model...\")\n",
        "  gpt2.download_gpt2(model_name = model_name)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "# 파라미터는 필요에 맞게 설정한다. \n",
        "# 본 실습에서는 빠른 진행을 위해 스텝(steps) 값을 100으로 설정하였다.\n",
        "gpt2.finetune(sess, data_file, model_name = model_name, \n",
        "              run_name = 'run1', print_every=1, sample_every=10, \n",
        "              save_every=50, learning_rate=0.0001, sample_length=500, \n",
        "              optimizer='adam', steps=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:11<00:00, 11.07s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1844986 tokens\n",
            "Training...\n",
            "[1 | 100.67] loss=3.00 avg=3.00\n",
            "[2 | 187.04] loss=3.27 avg=3.14\n",
            "[3 | 273.48] loss=3.30 avg=3.19\n",
            "[4 | 359.97] loss=3.22 avg=3.20\n",
            "[5 | 446.34] loss=3.55 avg=3.27\n",
            "[6 | 532.80] loss=3.16 avg=3.25\n",
            "[7 | 619.26] loss=3.61 avg=3.30\n",
            "[8 | 705.57] loss=3.35 avg=3.31\n",
            "[9 | 792.23] loss=3.05 avg=3.28\n",
            "[10 | 878.92] loss=3.11 avg=3.26\n",
            "======== SAMPLE 1 ========\n",
            " beneath a layer of mist. His skin suddenly became a shade of gold, his hair the color of a rose, his beard the color of roses of all varieties.\n",
            "\n",
            "The moment the sun rose, the sun rose with him. I could see with my own eyes that he would never leave his place in this world. He would be nothing more than a human skeleton.\n",
            "\n",
            "I did not even notice the sun until after the sun had set.\n",
            "\n",
            "If I was to say that he would die right here under the tree, then he would. Although his death would not be an instant or an explosion, it would be a terrible loss. In comparison to the many casualties he would suffer, that would be the most devastating.\n",
            "\n",
            "Before his sudden disappearance a second time, I was completely shocked by his sudden presence.\n",
            "\n",
            "\"Huu!\" (Makoto)\n",
            "\n",
            "Before I knew it, he was running on the spot. His long black fur had become his normal appearance.\n",
            "\n",
            "With the exception of his usual green eyes, I noticed that his eyes were not entirely black like his normal eyes. They were not bright or beautiful.\n",
            "\n",
            "While I watched the sun rise, I could not help but to feel relieved. Even if Makoto wasn't a supernatural being, he had made his presence a certainty to me. If he wasn't a human corpse, then he would definitely be a hero.\n",
            "\n",
            "But as the sun set, I knew that I would never see his face again.\n",
            "\n",
            "With his face barely recognizable, he made all the wrong decisions. He decided to give up being an adventurer and accept his role as an adventurer's leader. He felt obligated to leave his hometown of Alpaca, but the fact remains that he went through hardship along the way. To call it a tragedy is an understatement.\n",
            "\n",
            "Even with the help of his companions, he failed to come home. His family never accepted him and he was left as only an isolated example of why he's been treated as something of a savior.\n",
            "\n",
            "That being said, if a hero from an entirely different world were to save the world from a certain person from Alpaca, how do you know he would be able to withstand that situation?\n",
            "\n",
            "But the situation is so absurd that if this story had to be told, then that would be a good reason.\n",
            "\n",
            "Makoto was a mere human.\n",
            "\n",
            "But even though his mere existence was\n",
            "\n",
            "[11 | 1083.06] loss=3.30 avg=3.27\n",
            "[12 | 1171.25] loss=3.30 avg=3.27\n",
            "[13 | 1259.49] loss=3.26 avg=3.27\n",
            "[14 | 1347.44] loss=3.02 avg=3.25\n",
            "[15 | 1435.29] loss=3.37 avg=3.26\n",
            "[16 | 1523.21] loss=3.49 avg=3.27\n",
            "[17 | 1611.34] loss=3.29 avg=3.27\n",
            "[18 | 1700.26] loss=3.29 avg=3.27\n",
            "[19 | 1788.19] loss=3.16 avg=3.27\n",
            "[20 | 1875.72] loss=2.90 avg=3.25\n",
            "======== SAMPLE 1 ========\n",
            "ally was also an alcoholic , while her daughter was in and out of jail during the entire time period . But that's another story .\n",
            "\n",
            "\n",
            "I'm going to tell you one more time: If you don't have money . if you don't like the way someone dresses , if you don't think you can afford clothes , if you aren't happy with what you see on TV , and if you aren't sure if you're buying a costume for the right person ) , your daughter will likely have a very rough life . You know what she'll probably say? \" You know what? \"\n",
            "\n",
            "\n",
            "Don't try this at home .\n",
            "\n",
            "\n",
            "There is just too much to take in , too many potential reasons to want to give her some money for things she's going to be embarrassed to say that she wants , and too little time to do anything more serious with her life .<|endoftext|>This is the final year for this great idea !\n",
            "\n",
            "So, let's do what everybody else has already done - make the first five seasons of the first season of \"   -   -- | -  -- -  . . : \" \" I'll be home by 10 ; ) I'm an ordinary kid .\n",
            "\n",
            "\n",
            "A few weeks ago, I watched the new season of \"   -  -- | -  -- -  . \"  . \" \"  . \" \" , \" I'll be home by 10 . \" ) \" . \" ) \" . \" ) \" . \" ) \"  . \" , \" I'll be home by 10 . \" \" ) and I had a very good feeling . \" )\n",
            "\n",
            "\n",
            "But, when I saw the ending of \"   -  -- | -  -- -  . \"  . \" \" and I saw the music coming into the movie , I couldn't resist a little \" \" ) ) \" . \" ) \" . \" ) \" . \" ) \" . \" ) \" ) \" I felt like the \" , \" I'll be home by 10 \" ending was over the top .\n",
            "\n",
            "It feels too real . , just too right . . . there's something magical about the first 10 minutes , like a child taking a bite of his mother's food . . . \" ) ) \" ) ============== \"   -  -- | -  -- -  . .\" ] ! \" <sudden death \" scene 2 > : \" ) ! \" ) ============== \"\n",
            "\n",
            "[21 | 2076.49] loss=3.71 avg=3.27\n",
            "[22 | 2164.42] loss=3.24 avg=3.27\n",
            "[23 | 2252.32] loss=2.98 avg=3.26\n",
            "[24 | 2340.58] loss=3.04 avg=3.25\n",
            "[25 | 2428.99] loss=3.01 avg=3.24\n",
            "[26 | 2517.15] loss=3.04 avg=3.23\n",
            "[27 | 2605.50] loss=3.17 avg=3.22\n",
            "[28 | 2694.12] loss=3.42 avg=3.23\n",
            "[29 | 2782.23] loss=3.53 avg=3.24\n",
            "[30 | 2870.61] loss=3.57 avg=3.26\n",
            "======== SAMPLE 1 ========\n",
            "ite ,\n",
            "\n",
            "\" , one of the best features of the TV series , \n",
            "is how the characters are created . \n",
            "it's a show where it all just happens , just like in real life . \n",
            "there's no time to explain and sometimes , it's hard to see why one character is special while another isn't . \n",
            "there's an underlying sense that every character has just enough personality that viewers get to understand all the characters . \n",
            "if your viewing experience was so strong that you wanted the perfect experience , you should see this as a true success . \n",
            "some of the best moments from the series might come from the actual performances . \n",
            "the actors for this show may not just be their stars , but are often great role-models for other actors . \n",
            "there's nothing wrong with that . \n",
            "there's nothing wrong with making great actors , but it's very hard or impossible to get the right mix of personality and acting for a show . \n",
            "all this is what was lacking from earlier shows to the point that the actors were not able to play the \" right roles . \n",
            "you could say the same about the cast . \n",
            "you can't really get a great deal of acting out of just one actor . \n",
            "even the actors with very poor acting will play the roles they played in the previous series well . \n",
            "you can see all the qualities in this cast . \n",
            "all the way through the show , they were just great to watch . \n",
            "from the first episode , where it becomes clear something is wrong with my wife's wedding ceremony , to the latest one , where she is at the center of a big mystery for the audience , the actors are very good . \n",
            "one minute they're being clever and clever to each other and the next minute , they're playing to each other . \n",
            "with the exception of one actor , who got the benefit of over three seasons of acting on the same show , the rest of the cast of this show is just as good as they were in their earlier productions . \n",
            "as far as story is concerned , the episodes are very well narrated and they give each an equal amount of attention to the plot . \n",
            "it is not a typical television show , with a lot of action and a lot of action packed moments . \n",
            "there is a reason , for example , why the actors were able to deliver the \" biggest star turn in television history \" that they gave us .\n",
            "\n",
            "[31 | 3075.70] loss=3.16 avg=3.25\n",
            "[32 | 3164.68] loss=3.27 avg=3.25\n",
            "[33 | 3253.62] loss=3.28 avg=3.25\n",
            "[34 | 3342.22] loss=3.21 avg=3.25\n",
            "[35 | 3430.98] loss=2.98 avg=3.24\n",
            "[36 | 3519.50] loss=2.98 avg=3.23\n",
            "[37 | 3608.07] loss=3.17 avg=3.23\n",
            "[38 | 3696.45] loss=3.48 avg=3.24\n",
            "[39 | 3784.70] loss=3.33 avg=3.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6o7gmJMOUQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 훈련 모델을 저장하는 경우\n",
        "# 모델을 gdrive에 마운트한다.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoTT1jHLPuLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 체크포인트를 gdrive에 복사한다.\n",
        "gpt2.copy_checkpoint_to_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-A9F1SgP2nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 테스트(문장 생성)를 위해 글로벌 변수를 초기화한 후 문장을 생성 한다.\n",
        "# 훈련시 사용한 run_name을 명시하고 문장을 시작할 문자열을 정한다.\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "gpt2.generate(sess, run_name='run1', prefix= 'this movie')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1PmuRUBjrxC",
        "colab_type": "text"
      },
      "source": [
        "<참고>\n",
        "\n",
        "\n",
        "> https://github.com/minimaxir/gpt-2-simple\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQpZ_bIgjurx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}